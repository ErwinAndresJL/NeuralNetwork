# -*- coding: utf-8 -*-
"""NeuralNetworkByHand.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lu4Is_rfsNxZCATDUX8c_i8odIutjNey

# Use google drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import libraries"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
# Solamente para usuarios de Jupyter Themes
# from jupyterthemes import jtplot
# jtplot.style(grid=False)

"""# Import MNIST - characters written by hand"""

from get_images import get_images

"""# Save Path and get data"""

mnist_path = './mnist_raw/'

x_train_num, y_train_num, x_test_num, y_test_num = get_images(mnist_path)

x_test_num.shape

"""# Convert images in vectors float32"""

x_train = x_train_num[:50000].reshape(50000, -1).astype(np.float32)/255
y_train = y_train_num[:50000].reshape(50000, 1)

x_val = x_train_num[50000:].reshape(10000, -1).astype(np.float)/255
y_val = y_train_num[50000:].reshape(10000, 1)

x_test = x_test_num.copy().reshape(10000, -1).astype(np.float)/255
y_test = y_test_num.copy().reshape(10000, 1)

x_train.shape

"""# Plot some images"""

def plot_number(image):
  plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))
  plt.axis('off')
  plt.show()

rnd_idx = np.random.randint(len(y_test))
print(f'La imagen muestreada representa un: {y_test[rnd_idx]}')
plot_number(x_test_num[rnd_idx])

"""# Create MiniBatch"""

def create_minibatches(mb_size, x, y, shuffle = True):
  assert x.shape[0] == y.shape[0], 'error en cantidad de muestras'
  total_data = x.shape[0]
  if shuffle:
    idxs = np.arange(total_data)
    np.random.shuffle(idxs)
    x = x[idxs]
    y = y[idxs]

  return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range (0, total_data, mb_size))

"""# Init Parameters"""

def init_parameters(input_size, neurons):
  '''
  input size -> elementos de entrada, 784
  neurons -> list [] con cantidad de neuronas en cada capa

  mejores técnicas para redes más grandes
  '''

  W1 = np.random.randn(neurons[0], input_size) * 0.001
  b1 = np.zeros((neurons[0], 1))

  W2 = np.random.randn(neurons[1], neurons[0]) * 0.001
  b2 = np.zeros((neurons[1], 1))

  return {'W1': W1, 'b1':b1,'W2':W2, 'b2': b2}

parameters = init_parameters(28*28, [200, 10])
print(parameters['W1'].shape)
print(parameters['W2'].shape)
print(parameters['b1'].shape)
print(parameters['b2'].shape)

"""# Relu and Score Functions"""

def relu(x):
  return np.maximum(0, x)

def scores(x, parameters, activation_fcn):
  z1 = parameters['W1'] @ x + parameters['b1']
  a1 = activation_fcn(z1) # devuelve fcn de activación
  z2 = parameters['W2'] @ a1 + parameters['b2']

  return z2, z1, a1

scores, z1, a1 = scores(x_train[:64].T, parameters, relu)

x_train[:64].shape

"""# Softmax Function"""

def softmax(x):
  exp_scores = np.exp(x)
  sum_exp_scores = np.sum(exp_scores, axis=0)
  probs = exp_scores/sum_exp_scores
  return probs

"""# Loss Function - Crossentropy"""

def x_entropy(scores, y, batch_size=64):
  probs = softmax(scores)
  y_hat = probs[y.squeeze(), np.arange(batch_size)]
  cost = np.sum(-np.log(y_hat)) / batch_size

  return probs, cost

y_hat, cost = x_entropy(scores, y_train[:64])

y_hat.shape

"""# Backward Function"""

def backward(probs, x, y, z1, a1, parameters, batch_size=64):
  grads = {}
  probs[y.squeeze(), np.arange(batch_size)] -= 1 # y-hat - y
  dz2 = probs.copy()

  dW2 = dz2 @ a1.T / batch_size
  db2 = np.sum(dz2, axis = 1, keepdims = True) / batch_size
  da1 = parameters ['W2'].T @ dz2

  dz1 = da1.copy()
  dz1[z1 <= 0] = 0

  dW1 = dz1 @ x
  db1 = np.sum(dz1, axis=1, keepdims=True)

  assert parameters['W1'].shape == dW1.shape, 'W1 no es igual forma'
  assert parameters['W2'].shape == dW2.shape, 'W2 no es igual forma'
  assert parameters['b1'].shape == db1.shape, 'b1 no es igual forma'
  assert parameters['b2'].shape == db2.shape, 'b2 no es igual forma'

  grads = {'w1': dW1, 'b1':db1,'W2':dW2, 'b2': db2}

  return grads

grads = backward(y_hat, x_train[:64], y_train[:64], z1, a1, parameters)

"""# Accuracy Function"""

def accuracy(x_data, y_data, mb_size=64):
  correct=0
  total=0
  for i, (x, y) in enumerate(create_minibatches(mb_size, x_data, y_data)):
    scores2, z1, a1 = scores(x.T, parameters, relu)
    y_hat, cost = x_entropy(scores2, y, batch_size=len(x))

    correct += np.sum(np.argmax(y_hat, axis=0) == y.squeeze())
    total += y_hat.shape[1]
  return correct/total

"""# Train Function"""

def train (epochs, parameters, mb_size=64, learning_rate = 1e-3):
  for epoch in range(epochs):
    for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):
      scores2, z1, a1 = scores(x.T, parameters=parameters, activation_fcn=relu)
      y_hat, cost = x_entropy(scores2, y, batch_size=len(x))
      grads = backward(y_hat, x, y, z1, a1, parameters, batch_size=len(x))

      parameters['W1'] = parameters['W1'] - learning_rate*grads['w1']
      parameters['b1'] = parameters['b1'] - learning_rate*grads['b1']
      parameters['b2'] = parameters['b2'] - learning_rate*grads['b2']
      parameters['W2'] = parameters['W2'] - learning_rate*grads['W2']

    print(f'costo es: {cost}, y accuracy: {accuracy(x_val,y_val, mb_size)}')
  return parameters

mb_size = 512
learning_rate = 1e-2
epochs = 1
parameters = train(epochs=epochs, parameters=parameters, mb_size=mb_size, learning_rate=learning_rate)

"""# Datos de prueba"""

accuracy(x_train, y_train, mb_size)

accuracy(x_test, y_test, mb_size)

"""# Predicciones"""

def predict(x):
  scores2, _, _ = scores(x, parameters, relu)
  return np.argmax(scores2)

x_test[0].reshape(-1, 1).shape

idx = np.random.randint(len(y_test))
plot_number(x_test_num[idx])
pred = predict(x_test[idx].reshape(-1, 1))
print(f'el valor predicho es: {pred}')